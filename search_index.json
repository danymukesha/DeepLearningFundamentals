[["index.html", "Introduction to Deep Learning for Bioinformatics Welcome 0.1 Course Overview", " Introduction to Deep Learning for Bioinformatics Dany Mukesha January 8-9, 2025 Welcome This book contains the complete training materials for the “Introduction to Deep Learning” course. Date: December 8th &amp; 9th, 2025 Location: Chur (Remotely) 0.1 Course Overview In the first part of the course, we will cover the fundamental concepts and applications of deep learning, including model architectures, training procedures, and data preprocessing. In the second part, the focus will shift to applying deep learning models to bioinformatics tasks, using both custom-built and pre-trained models. 0.1.1 Learning Goals Getting an overview of the topic ‘Deep Learning’ (with bioinformatic examples). How to implement and train a neural network using Keras. Data preprocessing and encoding for neural network models. Large Language Models (LLMs) and pre-trained models and how they can be applied to various tasks. 0.1.2 Prerequisites Basic bioinformatics knowledge. Good knowledge of Python and the Linux Terminal. 0.1.3 Setup Instructions Please follow the instructions in the README.md file on the course’s GitHub repository to set up your environment before the course begins. This work is licensed under a Creative Commons Attribution 4.0 International License. "],["introduction-to-deep-learning.html", "Chapter 1 Introduction to Deep Learning 1.1 What is Deep Learning? 1.2 Why Deep Learning in Bioinformatics? 1.3 A Simple Biological Example: Transcription Factor Binding Prediction", " Chapter 1 Introduction to Deep Learning 1.1 What is Deep Learning? Deep Learning (DL) is a subfield of machine learning based on artificial neural networks. The “deep” refers to the number of layers through which the data is transformed. While a standard neural network might have 2-3 layers, a deep network can have dozens or hundreds. 1.1.1 Key Concepts Neuron: The basic computational unit. It takes inputs, multiplies them by weights, adds a bias, and passes the result through an activation function. Layer: A collection of neurons. Input Layer: The layer that receives the raw data. Hidden Layer(s): The layers between input and output where the “learning” happens. Output Layer: The final layer that produces the prediction. Activation Function: A function that determines the output of a neuron (e.g., ReLU, Sigmoid, Softmax). 1.2 Why Deep Learning in Bioinformatics? Bioinformatics data is often high-dimensional and complex, making it a perfect candidate for DL. Some examples: Genomics: Predicting the functional effect of non-coding variants, chromatin accessibility, and transcription factor binding. Proteomics: Predicting protein structure (e.g., AlphaFold), function, and interactions. Medical Imaging: Classifying tumors from histopathology images or MRI scans. Drug Discovery: Predicting molecular properties and drug-target interactions. 1.3 A Simple Biological Example: Transcription Factor Binding Prediction Imagine we have DNA sequences of length 100bp. Our goal is to predict whether a specific transcription factor (TF) binds to a given sequence. Input: \"ATCGATCGAT...\" (100 characters) Output: 1 (binds) or 0 (does not bind) We need to: Convert the sequence into a numerical format (more on this in Chapter 3). Design a neural network that can learn the binding motif and its context. In the following chapters, we will learn how to build such a model step-by-step. "],["implementing-neural-networks-with-keras.html", "Chapter 2 Implementing Neural Networks with Keras 2.1 What is Keras? 2.2 Your First Neural Network 2.3 Model Anatomy 2.4 Training the Model 2.5 Evaluating the Model 2.6 Exercise", " Chapter 2 Implementing Neural Networks with Keras 2.1 What is Keras? Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, JAX, or PyTorch. It was developed with a focus on enabling fast experimentation. 2.2 Your First Neural Network Let’s build a simple model to classify handwritten digits from the classic MNIST dataset. This is the “Hello, World!” of deep learning. # First, we import the necessary modules from Keras from tensorflow import keras from tensorflow.keras import layers # Load the MNIST dataset (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() # Preprocess the data: normalize pixel values to [0, 1] and flatten the 28x28 images x_train = x_train.astype(&quot;float32&quot;) / 255.0 x_test = x_test.astype(&quot;float32&quot;) / 255.0 x_train = x_train.reshape(-1, 784) # 28*28 = 784 x_test = x_test.reshape(-1, 784) # Build the model architecture model = keras.Sequential([ layers.Dense(128, activation=&#39;relu&#39;, input_shape=(784,)), # Input layer layers.Dense(64, activation=&#39;relu&#39;), # Hidden layer layers.Dense(10, activation=&#39;softmax&#39;) # Output layer (10 classes) ]) # Compile the model model.compile( optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) # Let&#39;s see the architecture model.summary() 2.3 Model Anatomy Sequential Model: A linear stack of layers. Dense Layer: A fully connected layer where each neuron is connected to every neuron in the previous layer. Activation Functions: relu (Rectified Linear Unit): Great for hidden layers. It helps with the vanishing gradient problem. softmax: Used in the output layer for multi-class classification. It outputs a probability distribution over the classes. Compilation: Optimizer: How the model updates its weights based on the loss. adam is a good default. Loss Function: The objective that the model tries to minimize. sparse_categorical_crossentropy is for integer labels. Metrics: What to monitor during training, like accuracy. 2.4 Training the Model # Train the model for 10 epochs (passes over the training data) history = model.fit( x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test) # Use test set for validation ) 2.5 Evaluating the Model # Evaluate on the test set test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2) print(f&quot;\\nTest accuracy: {test_accuracy:.4f}&quot;) 2.6 Exercise Run the code above. What is the final test accuracy? Try changing the number of neurons in the hidden layers (e.g., 256, 512). How does it affect the training time and accuracy? Try adding a third hidden layer. Does it improve performance? "],["data-preprocessing-for-bioinformatics.html", "Chapter 3 Data Preprocessing for Bioinformatics 3.1 Encoding Categorical Data 3.2 Handling Variable-Length Sequences 3.3 Data Generators", " Chapter 3 Data Preprocessing for Bioinformatics Neural networks require data to be in a numerical, normalized format. Biological data often requires special encoding. 3.1 Encoding Categorical Data 3.1.1 One-Hot Encoding for DNA Sequences This is the most common method for representing sequences. Each nucleotide is converted into a vector. A -&gt; [1, 0, 0, 0] T -&gt; [0, 1, 0, 0] C -&gt; [0, 0, 1, 0] G -&gt; [0, 0, 0, 1] N -&gt; [0, 0, 0, 0] A sequence \"ATCG\" becomes a 2D array of shape (4, 4). import numpy as np def one_hot_encode_dna(sequence): &quot;&quot;&quot;One-hot encode a DNA sequence.&quot;&quot;&quot; mapping = {&#39;A&#39;: [1,0,0,0], &#39;T&#39;: [0,1,0,0], &#39;C&#39;: [0,0,1,0], &#39;G&#39;: [0,0,0,1]} one_hot = [] for base in sequence: one_hot.append(mapping.get(base, [0,0,0,0])) # Handle &#39;N&#39; or other chars return np.array(one_hot) # Example seq = &quot;ATCGN&quot; encoded_seq = one_hot_encode_dna(seq) print(f&quot;Sequence: {seq}&quot;) print(f&quot;Encoded shape: {encoded_seq.shape}&quot;) print(encoded_seq) 3.2 Handling Variable-Length Sequences For sequences of different lengths, we use padding. We choose a maximum length and pad shorter sequences with zeros. from tensorflow.keras.preprocessing.sequence import pad_sequences # Assume `list_of_encoded_sequences` is a list of one-hot encoded sequences padded_sequences = pad_sequences(list_of_encoded_sequences, maxlen=200, padding=&#39;post&#39;, dtype=&#39;float32&#39;) print(f&quot;Padded sequences shape: {padded_sequences.shape}&quot;) # (num_samples, 200, 4) 3.3 Data Generators For large datasets that don’t fit in memory, we use tf.data.Dataset or Keras’ ImageDataGenerator to stream data from disk in batches. # Example for reading from text files def file_data_generator(file_paths, labels, batch_size=32): num_samples = len(file_paths) while True: # Loop forever so the generator never terminates for offset in range(0, num_samples, batch_size): batch_paths = file_paths[offset:offset+batch_size] batch_labels = labels[offset:offset+batch_size] batch_sequences = [] for path in batch_paths: with open(path, &#39;r&#39;) as f: seq = f.read().strip() encoded_seq = one_hot_encode_dna(seq) batch_sequences.append(encoded_seq) # Pad the sequences in the batch batch_x = pad_sequences(batch_sequences, maxlen=200, padding=&#39;post&#39;) yield batch_x, batch_labels # Usage # train_generator = file_data_generator(train_files, train_labels) # model.fit(train_generator, epochs=10, steps_per_epoch=len(train_files)//batch_size) "],["convolutional-neural-networks-for-genomics.html", "Chapter 4 Convolutional Neural Networks for Genomics 4.1 The Convolutional Layer 4.2 Case Study: Basset 4.3 Exercise: Build a TF Binding Predictor", " Chapter 4 Convolutional Neural Networks for Genomics Convolutional Neural Networks (CNNs) are not just for images. They are excellent at finding local, translation-invariant patterns, which makes them perfect for finding motifs in DNA sequences. 4.1 The Convolutional Layer A filter (or kernel) slides over the input sequence, computing a dot product at each position. A filter that recognizes the “TATA” box motif will have a high activation when it slides over a “TATA” sequence. from tensorflow.keras import layers, models # Assume input is a one-hot encoded sequence of length 200 -&gt; shape (200, 4) # We add a channel dimension: (200, 4, 1) to treat it like a 1D &quot;image&quot; input_seq = layers.Input(shape=(200, 4)) # Reshape to add a channel dimension x = layers.Reshape((200, 4, 1))(input_seq) # First 1D Convolutional Layer x = layers.Conv2D(filters=32, kernel_size=(10, 4), activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) # kernel_size=(10,4) means the filter is 10bp long and 4 nucleotides wide. # It can learn a motif of up to 10bp. x = layers.MaxPooling2D(pool_size=(2, 1))(x) # Reduce sequence length by half # Second Convolutional Layer x = layers.Conv2D(filters=64, kernel_size=(5, 1), activation=&#39;relu&#39;)(x) x = layers.MaxPooling2D(pool_size=(2, 1))(x) # Flatten and add Dense layers for classification x = layers.Flatten()(x) x = layers.Dense(128, activation=&#39;relu&#39;)(x) output = layers.Dense(1, activation=&#39;sigmoid&#39;)(x) # Binary classification model = models.Model(inputs=input_seq, outputs=output) model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() 4.2 Case Study: Basset Basset is a famous CNN architecture for predicting DNA accessibility from sequence [Kelley et al., 2016]. Its key features: * Multiple convolutional layers to learn hierarchical features (short motifs -&gt; combinations of motifs). * Multi-task learning: Predicts accessibility across hundreds of cell types simultaneously. * This improves generalization because the model learns features that are relevant across many tasks. 4.3 Exercise: Build a TF Binding Predictor Use the data from the DeepBind project. One-hot encode the provided DNA sequences. Build and train a CNN model similar to the one above. Try to visualize the first-layer filters to see if they have learned known TF motifs. "],["large-language-models-in-bioinformatics.html", "Chapter 5 Large Language Models in Bioinformatics 5.1 From Word Embeddings to Biological Embeddings 5.2 Pre-trained Models: ProtBERT and DNABERT 5.3 How to Use a Pre-trained Model 5.4 Potential Applications", " Chapter 5 Large Language Models in Bioinformatics 5.1 From Word Embeddings to Biological Embeddings Large Language Models (LLMs) like BERT and GPT work by learning deep contextual representations of words. We can apply the same principle to biological “words”. Word -&gt; k-mer: A short biological sequence (e.g., a 3-6 amino acid peptide or a 3-6 nucleotide k-mer). Sentence -&gt; Protein/DNA sequence: The entire sequence is a sentence made of k-mer words. 5.2 Pre-trained Models: ProtBERT and DNABERT Researchers have trained BERT-like models on massive corpora of protein sequences or DNA, creating powerful, general-purpose feature extractors for biology. ProtBERT: Trained on millions of protein sequences from UniProt. DNABERT: Trained on genomes from humans and other species. 5.3 How to Use a Pre-trained Model The typical workflow is transfer learning: Feature Extraction: Use the pre-trained model to convert your raw sequences into high-quality numerical features (embeddings). Then, train a simple classifier (e.g., SVM, Logistic Regression) on these features. Fine-tuning: Start with the pre-trained model and continue training it on your specific, smaller dataset. This adapts the model’s knowledge to your task. 5.3.1 Example: Using DNABERT for Promoter Prediction We will use the transformers library by Hugging Face. # First, install the library: pip install transformers from transformers import AutoTokenizer, AutoModel import torch # Load the pre-trained DNABERT model and tokenizer model_name = &quot;zhihan1996/DNABERT-2-117M&quot; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModel.from_pretrained(model_name) # Tokenize a DNA sequence sequence = &quot;ATCGATCGATCGATCG&quot; inputs = tokenizer(sequence, return_tensors=&quot;pt&quot;) # pt for PyTorch tensors # Get the model&#39;s embeddings with torch.no_grad(): outputs = model(**inputs) embeddings = outputs.last_hidden_state # This is the contextual embedding for each token # The [CLS] token&#39;s embedding is often used as a representation of the whole sequence whole_sequence_embedding = embeddings[0, 0, :] print(f&quot;Shape of sequence embeddings: {embeddings.shape}&quot;) print(f&quot;Shape of whole sequence embedding: {whole_sequence_embedding.shape}&quot;) You can now use whole_sequence_embedding as input to a classifier for your task (e.g., promoter vs. non-promoter). 5.4 Potential Applications Variant Effect Prediction: Feed the reference and alternate allele sequences to a model and see how the embedding changes. Protein Function Prediction: Use ProtBERT embeddings to predict Gene Ontology terms. Structure Prediction: Embeddings can be used as inputs to predict secondary structure. Note: Fine-tuning these models requires significant computational resources (GPUs). For this course, we focus on feature extraction. "],["advanced-topics-best-practices.html", "Chapter 6 Advanced Topics &amp; Best Practices 6.1 Regularization: Fighting Overfitting 6.2 Hyperparameter Tuning 6.3 Interpretability: What Did My Model Learn? 6.4 Resources for the Future", " Chapter 6 Advanced Topics &amp; Best Practices 6.1 Regularization: Fighting Overfitting When your model performs well on training data but poorly on validation data, it’s overfitting. Here’s how to combat it: Dropout: Randomly “drop out” (set to zero) a fraction of neurons during training. This prevents the network from becoming too reliant on any single neuron. model.add(layers.Dropout(0.5)) # Drop 50% of neurons L1/L2 Regularization: Add a penalty to the loss function for large weights, encouraging simpler models. from tensorflow.keras import regularizers model.add(layers.Dense(64, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01))) 6.2 Hyperparameter Tuning The choice of hyperparameters (learning rate, number of layers, dropout rate, etc.) is crucial. Use KerasTuner to automate the search. !pip install keras-tuner import kerastuner as kt def build_model(hp): model = keras.Sequential() model.add(layers.Flatten(input_shape=(784,))) # Tune the number of units in the first Dense layer hp_units = hp.Int(&#39;units&#39;, min_value=32, max_value=512, step=32) model.add(layers.Dense(units=hp_units, activation=&#39;relu&#39;)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(10, activation=&#39;softmax&#39;)) # Tune the learning rate hp_learning_rate = hp.Choice(&#39;learning_rate&#39;, values=[1e-2, 1e-3, 1e-4]) model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model tuner = kt.RandomSearch(build_model, objective=&#39;val_accuracy&#39;, max_trials=10) tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test)) best_model = tuner.get_best_models()[0] 6.3 Interpretability: What Did My Model Learn? Understanding why a model makes a prediction is critical in science. Saliency Maps: Compute the gradient of the output with respect to the input. This tells you which input positions (e.g., which nucleotides) were most important for the prediction. SHAP: A unified framework for interpreting model predictions. 6.4 Resources for the Future Keras Documentation &amp; Guides: The best resource. https://keras.io/ Bioconductor for R Users: The torch and tfestimators packages provide interfaces to deep learning frameworks in R. Community: Stay engaged with the Bioconductor and BioDL communities on Slack, GitHub, and at conferences. "],["references-and-further-reading.html", "Chapter 7 References and Further Reading 7.1 Books and Papers 7.2 Online Courses 7.3 Useful Links", " Chapter 7 References and Further Reading 7.1 Books and Papers Deep Learning with Python by François Chollet (Creator of Keras). A must-read. Kelley et al. (2016). “Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks.” Nature Methods. Ji et al. (2021). “DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome.” Bioinformatics. 7.2 Online Courses Prof. Andrew Ng’s Deep Learning Specialization on Coursera. Fast.ai: Practical Deep Learning for Coders. 7.3 Useful Links Keras: https://keras.io/ KerasHub: https://keras.io/keras_cv/ (for pre-trained computer vision models) Hugging Face: https://huggingface.co/ (for Transformers models like DNABERT) Bioconductor: https://bioconductor.org/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
